"""
model.eval()是为了区分诸如BN与Dropout的操作在训练与推理阶段的不同

Dropout:
    训练阶段随机丢弃一部分节点
    推理阶段使用全部节点

BN:
    提出背景
        深度神经网络之所以如此难训练，其中一个重要原因就是网络中层与层之间存在高度的关联性与耦合性。
        网络中层与层之间的关联性会导致如下的状况：随着训练的进行，网络中的参数也随着梯度下降在不停更新。
        上述这一现象叫做Internal Covariate Shift，影响如下：
        其一，上层参数需要不断适应新的输入数据分布，降低学习速度。
        其二，下层输入的变化可能趋向于变大或者变小，导致上层落入饱和区，使得学习过早停止。

    什么是Internal Covariate Shift
        Batch Normalization的原论文作者给了Internal Covariate Shift一个较规范的定义：在深层网络训练的过程中，由于网络中
        参数变化而引起内部结点数据分布发生变化的这一过程被称作Internal Covariate Shift。

        随着梯度下降的进行，每一层的参数 W与 b都会被更新，那么Z=Wx+b的分布也就发生了改变，进而a=g(z)(g为激活函数)也同样
        出现分布的改变。而a作为第 l+1 层的输入，意味着 L+1 层就需要去不停适应这种数据分布的变化，这一过程就被叫做
        Internal Covariate Shift。

    如何减缓Internal Covariate Shift？
        ICS产生的原因是由于参数更新带来的网络中每一层输入值分布的改变，并且随着网络层数的加深而变得更加严重，因此我们可
        以通过固定每一层网络输入值的分布来对减缓ICS问题。

        白化（Whitening）
            白化（Whitening）是机器学习里面常用的一种规范化数据分布的方法，主要是PCA白化与ZCA白化。白化是对输入数据分布
            进行变换，进而达到以下两个目的：
            去除特征之间的相关性 —> 独立；
            使得所有特征具有相同的均值和方差 —> 同分布。

        Batch Normalization提出
            白化主要有以下两个问题：
            白化过程计算成本太高
            白化过程由于改变了网络每一层的分布，因而改变了网络层中本身数据的表达能力。底层网络学习到的参数信息会被白化操作丢失掉。

            一方面，我们提出的normalization方法要能够简化计算过程；另一方面又需要经过规范化处理后让数据尽可能保留原始的
            表达能力。于是就有了简化+改进版的白化——Batch Normalization。

        Batch Normalization
            既然白化计算过程比较复杂，那我们就简化一点，比如我们可以尝试单独对每个特征进行normalizaiton就可以了，让每个
            特征都有均值为0，方差为1的分布就OK。

            另一个问题，既然白化操作减弱了网络中每一层输入数据表达能力，那我就再加个线性变换操作，让这些数据再能够尽可能
            恢复本身的表达能力就好了。

            在深度学习中，由于采用full batch的训练方式对内存要求较大，且每一轮训练时间过长；我们一般都会采用对数据做划分
            ，用mini-batch对网络进行训练。因此，Batch Normalization也就在mini-batch的基础上进行计算。

            BN算法：
            步骤一：
            avg = 1/m sum(Z) #m为batch-size, Z为wx + b
            var = 1/m sum( (Z - avg).pow(2) )
            Z_new = (Z - avg) / sqrt(var.pow(2) + \kesai) #\kesai是一个很小的正数，防止分母为0

            通过上面的变换，我们解决了第一个问题，即用更加简化的方式来对数据进行规范化，使得第l层的输入每个特征的分布均
            值为0，方差为1。

            如同上面提到的，Normalization操作我们虽然缓解了ICS问题，让每一层网络的输入数据分布都变得稳定，但却导致了数据
            表达能力的缺失。也就是我们通过变换操作改变了原有数据的信息表达（representation ability of the network），使
            得底层网络学习到的参数信息丢失。

            步骤二：
            因此，BN又引入了两个可学习（learnable）的参数 \lamda与 \beta。这两个参数的引入是为了恢复数据本身的表达能力，
            对规范化后的数据进行线性变换，即 Z_new = \lambda * Z_new + \beta。

            通过上面的步骤，我们就在一定程度上保证了输入数据的表达能力。

        测试阶段如何使用Batch Normalization？
            我们知道BN在每一层计算的 [公式] 与 [公式] 都是基于当前batch中的训练数据，但是这就带来了一个问题：我们在预测
            阶段，有可能只需要预测一个样本或很少的样本，没有像训练样本中那么多的数据，此时 [公式] 与 [公式] 的计算一定
            是有偏估计，这个时候我们该如何进行计算呢？

            利用BN训练好模型后，我们保留了每组mini-batch训练数据在网络中每一层的 avg与 var。此时我们使用整个样本的统计
            量来对Test数据进行归一化，具体来说使用均值与方差的无偏估计：
                avg_test = E(avg)
                var_test = m/(m-1) E(var)

            另外，除了采用整体样本的无偏估计外。吴恩达在Coursera上的Deep Learning课程指出可以对train阶段每个batch计算的
            mean/variance采用指数加权平均来得到test阶段mean/variance的估计。

        Batch Normalization的优势:
            BN使得网络中每层输入数据的分布相对稳定，加速模型学习速度
            BN使得模型对网络中的参数不那么敏感，简化调参过程，使得网络学习更加稳定
            BN允许网络使用饱和性激活函数（例如sigmoid，tanh等），缓解梯度消失问题
            BN具有一定的正则化效果
"""